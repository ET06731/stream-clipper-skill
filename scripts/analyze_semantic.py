#!/usr/bin/env python3
"""
å­—å¹•è¯­ä¹‰åˆ†ææ¨¡å—
è§£æå­—å¹•å†…å®¹ï¼Œè¯†åˆ«è¯é¢˜ç»“æ„å’Œç²¾å½©ç‚¹
"""

import re
import json
from pathlib import Path
from typing import List, Dict, Tuple
from dataclasses import dataclass


@dataclass
class SubtitleEntry:
    """å­—å¹•æ¡ç›®"""

    start: float
    end: float
    text: str
    index: int


@dataclass
class Segment:
    """è¯­ä¹‰æ®µè½"""

    start: float
    end: float
    text: str
    topic: str
    excitement_score: int  # 1-5
    key_quotes: List[str]
    keywords: List[str]


class SemanticAnalyzer:
    """å­—å¹•è¯­ä¹‰åˆ†æå™¨"""

    # æƒ…ç»ªå…³é”®è¯æ˜ å°„
    EMOTION_KEYWORDS = {
        "excited": [
            "å“ˆå“ˆ",
            "ç¬‘æ­»",
            "å§æ§½",
            "ç‰›é€¼",
            "å¤ªå¼ºäº†",
            "ç¥",
            "ååœºé¢",
            "ç»å…¸",
            "éœ‡æ’¼",
            "æƒŠè®¶",
        ],
        "funny": ["å“ˆå“ˆ", "hhh", "ç¬‘", "è‰", "233", " funny", " hilarious"],
        "shocked": ["ä»€ä¹ˆ", "ä¸å¯èƒ½", "çœŸçš„å‡çš„", "æƒŠ", "å“", "???", "ï¼Ÿ"],
        "angry": ["æ°”", "æ€’", "è®¨åŒ", "çƒ¦", "æ»š", "å¯æ¶"],
        "sad": ["å“­", "æ³ª", "éš¾å—", "å¿ƒç–¼", " sad", "æ³ªç›®"],
    }

    # è¯é¢˜è½¬æ¢æ ‡è®°è¯
    TOPIC_TRANSITIONS = [
        "æ¥ä¸‹æ¥",
        "ç„¶å",
        "é‚£ä¹ˆ",
        "å¥½äº†",
        "æ¥ä¸‹æ¥æˆ‘ä»¬",
        "ç°åœ¨",
        "next",
        "so",
        "alright",
        "okay then",
        "moving on",
    ]

    def __init__(self):
        self.subtitles: List[SubtitleEntry] = []

    def parse_srt(self, srt_path: str) -> List[SubtitleEntry]:
        """è§£æ SRT å­—å¹•æ–‡ä»¶"""
        entries = []

        with open(srt_path, "r", encoding="utf-8") as f:
            content = f.read()

        # åˆ†å‰²å­—å¹•å—
        blocks = re.split(r"\n\n+", content.strip())

        for block in blocks:
            lines = block.strip().split("\n")
            if len(lines) < 3:
                continue

            # è§£æåºå·
            try:
                index = int(lines[0])
            except ValueError:
                continue

            # è§£ææ—¶é—´æˆ³
            time_line = lines[1]
            time_match = re.match(
                r"(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})", time_line
            )
            if not time_match:
                continue

            start = self._time_to_seconds(time_match.group(1))
            end = self._time_to_seconds(time_match.group(2))

            # è§£ææ–‡æœ¬
            text = " ".join(lines[2:])

            entries.append(SubtitleEntry(start=start, end=end, text=text, index=index))

        return entries

    def _time_to_seconds(self, time_str: str) -> float:
        """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’"""
        # æ ¼å¼: 00:00:00,000
        parts = time_str.replace(",", ".").split(":")
        return int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2])

    def detect_topic_changes(self, entries: List[SubtitleEntry]) -> List[int]:
        """
        æ£€æµ‹è¯é¢˜è½¬æ¢ç‚¹
        è¿”å›è½¬æ¢ç‚¹çš„ç´¢å¼•åˆ—è¡¨
        """
        change_points = [0]  # å¼€å¤´æ€»æ˜¯è½¬æ¢ç‚¹

        for i in range(1, len(entries)):
            current_text = entries[i].text.lower()

            # 1. æ£€æŸ¥è¯é¢˜è½¬æ¢æ ‡è®°è¯
            for marker in self.TOPIC_TRANSITIONS:
                if marker in current_text:
                    change_points.append(i)
                    break

            # 2. æ£€æŸ¥é•¿æ—¶é—´é—´éš” (> 2ç§’)
            time_gap = entries[i].start - entries[i - 1].end
            if time_gap > 2.0:
                change_points.append(i)

        # å»é‡å¹¶æ’åº
        change_points = sorted(set(change_points))

        return change_points

    def extract_keywords(self, text: str, top_n: int = 5) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€å•çš„é¢‘ç‡ç»Ÿè®¡ï¼‰"""
        from collections import Counter

        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆåŸºäºå­—ç¬¦ï¼‰
        words = []
        for i in range(len(text) - 1):
            for j in range(2, min(5, len(text) - i + 1)):
                word = text[i : i + j]
                # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
                if len(word) >= 2 and not word.isdigit():
                    words.append(word)

        word_freq = Counter(words)

        # è¿‡æ»¤å¸¸è§åœç”¨è¯
        stop_words = {"è¿™ä¸ª", "é‚£ä¸ª", "ä»€ä¹ˆ", "ä¸€ä¸ª", "å¯ä»¥", "å°±æ˜¯", "æˆ‘ä»¬", "ä½ ä»¬"}
        for sw in stop_words:
            if sw in word_freq:
                del word_freq[sw]

        return [word for word, _ in word_freq.most_common(top_n)]

    def calculate_excitement(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„å…´å¥‹åº¦è¯„åˆ† (1-5)
        åŸºäºæƒ…ç»ªå…³é”®è¯å¯†åº¦
        """
        text_lower = text.lower()
        score = 1

        # ç»Ÿè®¡å„ç±»æƒ…ç»ªè¯
        emotion_counts = {}
        for emotion, keywords in self.EMOTION_KEYWORDS.items():
            count = sum(1 for kw in keywords if kw in text_lower)
            emotion_counts[emotion] = count

        # æ ¹æ®æƒ…ç»ªè¯æ•°é‡è¯„åˆ†
        total_emotion_words = sum(emotion_counts.values())

        if total_emotion_words >= 5:
            score = 5
        elif total_emotion_words >= 3:
            score = 4
        elif total_emotion_words >= 2:
            score = 3
        elif total_emotion_words >= 1:
            score = 2

        return score

    def extract_key_quotes(self, entries: List[SubtitleEntry]) -> List[str]:
        """æå–å…³é”®è¯­å½•/åè¨€"""
        quotes = []

        for entry in entries:
            text = entry.text

            # 1. åŒ…å«å¼ºçƒˆæƒ…ç»ªè¯çš„å¥å­
            if any(kw in text for kw in self.EMOTION_KEYWORDS["excited"]):
                # æå–åŒ…å«æƒ…ç»ªè¯çš„å®Œæ•´å¥å­
                quotes.append(text)

            # 2. åŒ…å«æ„Ÿå¹å·çš„å¥å­ï¼ˆé€šå¸¸è¡¨è¾¾å¼ºçƒˆæƒ…ç»ªï¼‰
            elif "ï¼" in text or "!" in text:
                if len(text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„
                    quotes.append(text)

            # 3. åŒ…å«é—®å·çš„ç–‘é—®å¥ï¼ˆå¯èƒ½æœ‰æ¢—ï¼‰
            elif "ï¼Ÿ" in text or "?" in text:
                if any(kw in text for kw in ["ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆ", "çœŸçš„"]):
                    quotes.append(text)

        # å»é‡å¹¶é™åˆ¶æ•°é‡
        quotes = list(dict.fromkeys(quotes))[:10]

        return quotes

    def generate_topic(self, text: str) -> str:
        """
        åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆè¯é¢˜æ ‡ç­¾
        ç®€å•çš„å¯å‘å¼æ–¹æ³•
        """
        text_lower = text.lower()

        # è¯é¢˜å…³é”®è¯æ˜ å°„
        topic_keywords = {
            "ç¼–ç¨‹": ["ä»£ç ", "ç¨‹åº", "ç¼–ç¨‹", "python", "javascript", "bug", "æŠ¥é”™"],
            "æ¸¸æˆ": ["æ¸¸æˆ", "æ‰“", "ç©", "é€šå…³", "boss", "å…³å¡", "æ¸¸æˆ"],
            "èŠå¤©": ["èŠå¤©", "è¯´", "è®²", "èŠ", "è¯é¢˜", "è®¨è®º"],
            "å”±æ­Œ": ["å”±æ­Œ", "æ­Œ", "å”±", "éŸ³ä¹"],
            "æç¬‘": ["ç¬‘", "æç¬‘", "å“ˆå“ˆå“ˆ", "æ¢—", "æ®µå­"],
        }

        # åŒ¹é…è¯é¢˜
        topic_scores = {}
        for topic, keywords in topic_keywords.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            if score > 0:
                topic_scores[topic] = score

        if topic_scores:
            return max(topic_scores, key=topic_scores.get)
        else:
            return "ç›´æ’­äº’åŠ¨"

    def analyze(self, subtitle_path: str) -> Dict:
        """
        å®Œæ•´åˆ†ææµç¨‹

        Returns:
            {
                'total_subtitles': int,
                'total_duration': float,
                'segments': [
                    {
                        'start': float,
                        'end': float,
                        'topic': str,
                        'excitement_score': int,
                        'key_quotes': [str],
                        'keywords': [str]
                    }
                ],
                'highlights': [
                    {
                        'start': float,
                        'end': float,
                        'reason': str,
                        'score': int
                    }
                ]
            }
        """
        print(f"ğŸ“– åˆ†æå­—å¹•æ–‡ä»¶: {Path(subtitle_path).name}")

        # è§£æå­—å¹•
        self.subtitles = self.parse_srt(subtitle_path)
        print(f"   æ€»å­—å¹•æ•°: {len(self.subtitles)}")

        if not self.subtitles:
            return {
                "total_subtitles": 0,
                "total_duration": 0,
                "segments": [],
                "highlights": [],
            }

        # æ£€æµ‹è¯é¢˜è½¬æ¢ç‚¹
        change_points = self.detect_topic_changes(self.subtitles)
        print(f"   æ£€æµ‹åˆ° {len(change_points)} ä¸ªè¯é¢˜è½¬æ¢ç‚¹")

        # åˆ†æ®µåˆ†æ
        segments = []
        for i in range(len(change_points)):
            start_idx = change_points[i]
            end_idx = (
                change_points[i + 1]
                if i + 1 < len(change_points)
                else len(self.subtitles)
            )

            segment_entries = self.subtitles[start_idx:end_idx]

            if not segment_entries:
                continue

            # åˆå¹¶æ–‡æœ¬
            segment_text = " ".join(e.text for e in segment_entries)

            # åˆ†ææ®µè½
            segment = Segment(
                start=segment_entries[0].start,
                end=segment_entries[-1].end,
                text=segment_text[:200] + ("..." if len(segment_text) > 200 else ""),
                topic=self.generate_topic(segment_text),
                excitement_score=self.calculate_excitement(segment_text),
                key_quotes=self.extract_key_quotes(segment_entries),
                keywords=self.extract_keywords(segment_text),
            )

            segments.append(segment)

        # è¯†åˆ«ç²¾å½©ç‰‡æ®µ (å…´å¥‹åº¦ >= 4)
        highlights = []
        for segment in segments:
            if segment.excitement_score >= 4:
                highlights.append(
                    {
                        "start": segment.start,
                        "end": segment.end,
                        "reason": f"é«˜æƒ…ç»ªååº” (è¯„åˆ†: {segment.excitement_score}/5)",
                        "score": segment.excitement_score,
                        "key_quotes": segment.key_quotes[:3],
                        "topic": segment.topic,
                    }
                )

        # æŒ‰è¯„åˆ†æ’åº
        highlights.sort(key=lambda x: x["score"], reverse=True)

        result = {
            "total_subtitles": len(self.subtitles),
            "total_duration": self.subtitles[-1].end if self.subtitles else 0,
            "segments": [
                {
                    "start": s.start,
                    "end": s.end,
                    "topic": s.topic,
                    "excitement_score": s.excitement_score,
                    "key_quotes": s.key_quotes,
                    "keywords": s.keywords,
                }
                for s in segments
            ],
            "highlights": highlights[:10],  # Top 10
        }

        # ä¿å­˜ç»“æœ
        output_path = Path(subtitle_path).with_suffix(".semantic_analysis.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸŒŸ ç²¾å½©ç‰‡æ®µ:")
        for i, hl in enumerate(highlights[:5], 1):
            start_min = int(hl["start"] // 60)
            start_sec = int(hl["start"] % 60)
            print(
                f"   {i}. [{start_min:02d}:{start_sec:02d}] {hl['topic']} - {hl['reason']}"
            )
            if hl["key_quotes"]:
                print(f'      "{hl["key_quotes"][0][:50]}..."')

        return result


def main():
    import argparse

    parser = argparse.ArgumentParser(description="åˆ†æå­—å¹•è¯­ä¹‰")
    parser.add_argument("subtitle_file", help="å­—å¹• SRT æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    analyzer = SemanticAnalyzer()
    result = analyzer.analyze(args.subtitle_file)

    print(f"\nâœ… åˆ†æå®Œæˆ! è¯†åˆ« {len(result['highlights'])} ä¸ªç²¾å½©ç‰‡æ®µ")


if __name__ == "__main__":
    main()
