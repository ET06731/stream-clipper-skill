#!/usr/bin/env python3
"""
åˆ†æå¼¹å¹•å¯†åº¦ï¼Œè¯†åˆ«é«˜äº’åŠ¨æ—¶é—´ç‚¹
"""

import re
import json
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict
import xml.etree.ElementTree as ET


class DanmakuAnalyzer:
    """å¼¹å¹•åˆ†æå™¨"""

    def __init__(self, window_size: int = 30):
        """
        Args:
            window_size: æ—¶é—´çª—å£å¤§å°ï¼ˆç§’ï¼‰
        """
        self.window_size = window_size

    def parse_danmaku_xml(self, xml_path: str) -> List[Dict]:
        """è§£æ Bç«™å¼¹å¹• XML æ–‡ä»¶"""
        danmaku_list = []

        try:
            tree = ET.parse(xml_path)
            root = tree.getroot()

            for d in root.findall("d"):
                # å¼¹å¹•å±æ€§: p="æ—¶é—´,æ¨¡å¼,å­—å·,é¢œè‰²,æ—¶é—´æˆ³,å¼¹å¹•æ± ,ç”¨æˆ·ID,rowID"
                p = d.get("p", "").split(",")
                if len(p) >= 8:
                    time_sec = float(p[0])
                    danmaku_list.append(
                        {
                            "time": time_sec,
                            "text": d.text or "",
                            "mode": int(p[1]),
                            "user": p[6],
                        }
                    )

        except Exception as e:
            print(f"è§£æå¼¹å¹•å¤±è´¥: {e}")

        return sorted(danmaku_list, key=lambda x: x["time"])

    def calculate_density(self, danmaku_list: List[Dict]) -> Dict:
        """
        è®¡ç®—å¼¹å¹•å¯†åº¦åˆ†å¸ƒ

        Returns:
            {
                'windows': [
                    {'start': float, 'end': float, 'count': int, 'users': set}
                ],
                'avg_density': float,
                'peak_windows': [...]
            }
        """
        if not danmaku_list:
            return {"windows": [], "avg_density": 0, "peak_windows": []}

        # æŒ‰æ—¶é—´çª—å£åˆ†ç»„
        windows = defaultdict(lambda: {"count": 0, "users": set(), "texts": []})

        for d in danmaku_list:
            window_start = int(d["time"] // self.window_size) * self.window_size
            windows[window_start]["count"] += 1
            windows[window_start]["users"].add(d["user"])
            windows[window_start]["texts"].append(d["text"])

        # è½¬æ¢ä¸ºåˆ—è¡¨
        window_list = []
        for start in sorted(windows.keys()):
            window_list.append(
                {
                    "start": start,
                    "end": start + self.window_size,
                    "count": windows[start]["count"],
                    "unique_users": len(windows[start]["users"]),
                    "texts": windows[start]["texts"],
                }
            )

        # è®¡ç®—å¹³å‡å¯†åº¦
        avg_density = (
            sum(w["count"] for w in window_list) / len(window_list)
            if window_list
            else 0
        )

        # è¯†åˆ«å³°å€¼çª—å£ (å¯†åº¦ > å¹³å‡å€¼çš„ 1.5 å€)
        peak_windows = [w for w in window_list if w["count"] > avg_density * 1.5]
        peak_windows.sort(key=lambda x: x["count"], reverse=True)

        return {
            "windows": window_list,
            "avg_density": avg_density,
            "peak_windows": peak_windows[:10],  # Top 10
        }

    def extract_keywords(self, texts: List[str], top_n: int = 10) -> List[str]:
        """æå–é«˜é¢‘å…³é”®è¯"""
        from collections import Counter

        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆåŸºäºå­—ç¬¦ï¼‰
        word_count = Counter()

        for text in texts:
            if not text:
                continue
            # 2-4 å­—ç¬¦çš„è¯
            for i in range(len(text) - 1):
                for j in range(2, min(5, len(text) - i + 1)):
                    word = text[i : i + j]
                    if len(word) >= 2:
                        word_count[word] += 1

        # è¿‡æ»¤å¸¸è§åœç”¨è¯
        stop_words = {
            "å“ˆå“ˆ",
            "å•Šå•Š",
            "ä»€ä¹ˆ",
            "è¿™ä¸ª",
            "é‚£ä¸ª",
            "ä»Šå¤©",
            "ç°åœ¨",
            "çœŸçš„",
            "å¯ä»¥",
        }
        for sw in stop_words:
            if sw in word_count:
                del word_count[sw]

        return [word for word, _ in word_count.most_common(top_n)]

    def analyze(self, danmaku_path: str) -> Dict:
        """
        å®Œæ•´åˆ†ææµç¨‹

        Returns:
            {
                'total_danmaku': int,
                'total_users': int,
                'avg_density': float,
                'peak_moments': [
                    {
                        'start': float,
                        'end': float,
                        'density': int,
                        'keywords': [str]
                    }
                ]
            }
        """
        print(f"ğŸ“Š åˆ†æå¼¹å¹•æ–‡ä»¶: {Path(danmaku_path).name}")

        # è§£æå¼¹å¹•
        danmaku_list = self.parse_danmaku_xml(danmaku_path)
        print(f"   æ€»å¼¹å¹•æ•°: {len(danmaku_list)}")

        if not danmaku_list:
            return {
                "total_danmaku": 0,
                "total_users": 0,
                "avg_density": 0,
                "peak_moments": [],
            }

        # è®¡ç®—å¯†åº¦
        density_result = self.calculate_density(danmaku_list)

        # ç»Ÿè®¡ç”¨æˆ·
        all_users = set(d["user"] for d in danmaku_list)

        # å¤„ç†å³°å€¼æ—¶åˆ»
        peak_moments = []
        for window in density_result["peak_windows"]:
            keywords = self.extract_keywords(window["texts"], 5)
            peak_moments.append(
                {
                    "start": window["start"],
                    "end": window["end"],
                    "density": window["count"],
                    "unique_users": window["unique_users"],
                    "keywords": keywords,
                }
            )

        result = {
            "total_danmaku": len(danmaku_list),
            "total_users": len(all_users),
            "avg_density": density_result["avg_density"],
            "window_size": self.window_size,
            "peak_moments": peak_moments,
        }

        # ä¿å­˜ç»“æœ
        output_path = Path(danmaku_path).with_suffix(".danmaku_analysis.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ”¥ é«˜å¯†åº¦æ—¶æ®µ:")
        for i, moment in enumerate(peak_moments[:5], 1):
            start_min = moment["start"] // 60
            start_sec = moment["start"] % 60
            print(
                f"   {i}. [{start_min:02d}:{start_sec:02d}] å¯†åº¦: {moment['density']}æ¡/30ç§’"
            )
            print(f"      å…³é”®è¯: {', '.join(moment['keywords'])}")

        return result


def main():
    import argparse

    parser = argparse.ArgumentParser(description="åˆ†æå¼¹å¹•å¯†åº¦")
    parser.add_argument("danmaku_file", help="å¼¹å¹• XML æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--window", "-w", type=int, default=30, help="æ—¶é—´çª—å£ï¼ˆç§’ï¼‰")

    args = parser.parse_args()

    analyzer = DanmakuAnalyzer(window_size=args.window)
    result = analyzer.analyze(args.danmaku_file)

    print(f"\nâœ… åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜")


if __name__ == "__main__":
    main()
